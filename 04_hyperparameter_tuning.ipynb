{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNt-yRqI59SA"
   },
   "source": [
    "# Notebook Description\n",
    "Initial notebook for larger analysis of multiple reruns. Includes random search hyperparameter tuning, along with evaluating over multiple reruns for all data types: baseline, random and balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions.nn import create_nn\n",
    "from helper_functions.notebook_utils import BinaryExpectedCalibrationError, balance_classes, encode_and_split_new, equate_weights, generate_one_hot, paired_t_test, train_simultaneously, two_sample_t_test, verify_proportions\n",
    "from helper_functions.testing import encode_and_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "0m6ux8HiGPQB",
    "outputId": "fa74c23e-8f1a-432c-9371-cb363ad4ad8c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import ttest_rel\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "import keras.backend as K\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import h5py\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "es12xKuscu7n",
    "outputId": "bae4be81-24c2-4435-d93a-b1700735b33d"
   },
   "outputs": [],
   "source": [
    "file_path = 'combined_w_age_and_insurance.csv'  # Update with your file path\n",
    "\n",
    "try:\n",
    "  merged_df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "  print(f\"Error: File not found at {file_path}\")\n",
    "except pd.errors.ParserError:\n",
    "  print(f\"Error: Unable to parse the CSV file at {file_path}. Check the file format.\")\n",
    "except Exception as e:\n",
    "  print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "merged_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "bhspqlbmGkSZ",
    "outputId": "80e83b2a-2499-4b75-baa2-80cc0c69e728"
   },
   "outputs": [],
   "source": [
    "frequent_races = ['WHITE', 'BLACK', 'HISPANIC', 'ASIAN']\n",
    "\n",
    "# Update 'race' column\n",
    "merged_df.loc[~merged_df['race'].isin(frequent_races), 'race'] = 'OTHER'\n",
    "\n",
    "columns = merged_df.columns\n",
    "\n",
    "numerical_columns = list(merged_df.columns[5:13])\n",
    "numerical_columns.append('age')\n",
    "\n",
    "for column in numerical_columns:\n",
    "    scaler = MinMaxScaler()\n",
    "    values = merged_df[column].values.reshape(-1, 1)\n",
    "    merged_df[column] = scaler.fit_transform(values)\n",
    "\n",
    "copy = merged_df.copy()\n",
    "\n",
    "categorical_columns = ['gender', 'arrival_transport', 'race']\n",
    "numeric_values = copy[numerical_columns].values\n",
    "categorical_values = copy[categorical_columns]\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the OneHotEncoder with the categorical values\n",
    "encoder.fit(categorical_values)\n",
    "\n",
    "categorical_values = encoder.transform(categorical_values).toarray()\n",
    "\n",
    "X_num_cat = np.concatenate((numeric_values, categorical_values), axis=1)\n",
    "\n",
    "disposition_column = merged_df['disposition'].values\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit LabelEncoder and transform the 'disposition' column\n",
    "Y_encoded = label_encoder.fit_transform(disposition_column)\n",
    "Y = (1 - Y_encoded)\n",
    "\n",
    "text = merged_df['chiefcomplaint'].values\n",
    "\n",
    "# Combining text data with numerical data for training and testing\n",
    "string_array_reshaped = text.reshape(-1, 1)\n",
    "X_combined = np.concatenate((string_array_reshaped, X_num_cat), axis=1)\n",
    "\n",
    "races = ['ASIAN', 'BLACK', 'HISPANIC', 'OTHER', 'WHITE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k24vqWOHGxU3",
    "outputId": "6a58a6ff-dbd2-4e27-835e-4ec5584f405e"
   },
   "outputs": [],
   "source": [
    "for i in range(len(merged_df)):\n",
    "    row = X_combined[i, -5:]\n",
    "    for j in range(row.shape[0]):\n",
    "        if row[j] == 1:\n",
    "            ind = j\n",
    "    if ind != races.index(merged_df['race'].loc[i]):\n",
    "        print('Something went wrong')\n",
    "\n",
    "proportions = []\n",
    "\n",
    "for race in races:\n",
    "    print(f'Race: {race}')\n",
    "    pct = (len(merged_df[merged_df['race']==race]))/(len(merged_df))\n",
    "    proportions.append(pct)\n",
    "    print(f'- {round(pct*100, 2)}% of dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOiDsTsvsdSr"
   },
   "source": [
    "# Baseline Model Hyperparameter Tuning & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "So7rViBtDsmp"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_combined, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dictionary of array masks to correspond which races are at each index for easier computation\n",
    "masks = {race: None for race in races}\n",
    "for i in range(len(races)):\n",
    "    masks[races[i]] = X_train[:, -5+i] == 1\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_combined, Y, test_size=0.3)\n",
    "\n",
    "Y_train = Y_train.astype('float32')\n",
    "Y_val = Y_val.astype('float32')\n",
    "\n",
    "X_num_train, X_num_val, X_enc_train, X_enc_val = encode_and_split_new(X_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUN5RBmJsjzA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def create_model(shape_text, shape_num, layer1_values, layer2_values, num_bins=10, dropout=0):\n",
    "    initializer = GlorotUniform()\n",
    "\n",
    "    # Define input shapes\n",
    "    text_input = Input(shape=(shape_text,), name='encoded_text_input')\n",
    "    combined_input = Input(shape=(shape_num,), name='numeric_input')\n",
    "\n",
    "    # Define neural network for text data with optional dropout\n",
    "    text_model = text_input\n",
    "    for units in layer1_values:\n",
    "        text_model = Dense(units, activation='relu', kernel_initializer=initializer)(text_model)\n",
    "        if dropout > 0:\n",
    "            text_model = Dropout(dropout)(text_model)\n",
    "\n",
    "    # Concatenate text model output with combined numerical/categorical input\n",
    "    combined_with_text = Concatenate()([combined_input, text_model])\n",
    "\n",
    "    # Define additional layers if needed with optional dropout\n",
    "    for units in layer2_values:\n",
    "        combined_with_text = Dense(units, activation='relu', kernel_initializer=initializer)(combined_with_text)\n",
    "        if dropout > 0:\n",
    "            combined_with_text = Dropout(dropout)(combined_with_text)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, activation='sigmoid', kernel_initializer=initializer)(combined_with_text)\n",
    "\n",
    "    # Define model\n",
    "    model = Model(inputs=[combined_input, text_input], outputs=output)\n",
    "\n",
    "    loss = BinaryExpectedCalibrationError(num_bins=num_bins)\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy', AUC(name='val_auc')])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Custom callback to track overall best AUC across all iterations\n",
    "class PrintOnBestAUC(Callback):\n",
    "    def __init__(self, best_auc_global_ref, min_diff = 0.001):\n",
    "        super().__init__()\n",
    "        self.best_auc_global_ref = best_auc_global_ref\n",
    "        self.min_diff = min_diff\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_auc = logs.get(\"val_auc\")\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        # Check against global best AUC stored in the list\n",
    "        if current_auc > self.best_auc_global_ref[0] + self.min_diff:\n",
    "            print(f\" Best Validation Set AUC improved from {self.best_auc_global_ref[0]} to {current_auc}\")\n",
    "            self.best_auc_global_ref[0] = current_auc\n",
    "            print(f\"  Training loss: {logs['loss']:.4f} - accuracy: {logs['accuracy']:.4f}\")\n",
    "            print(f\"  Validation loss: {logs['val_loss']:.4f} - accuracy: {logs['val_accuracy']:.4f} - AUC: {logs['val_auc']:.4f}\")\n",
    "            # Update best model when global best_auc improves\n",
    "            print(\"Saving best model...\\n\")\n",
    "            model.save('best_model.keras')  # Save only when overall best AUC improves\n",
    "\n",
    "        else:\n",
    "            print(f\"AUC did not improve from {self.best_auc_global_ref[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzFSveDtKCPn",
    "outputId": "3699d0cc-f044-40f6-d3f1-e38af3937da9"
   },
   "outputs": [],
   "source": [
    "# Check if TensorFlow is using GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Verify if TensorFlow is using the GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-Cz5_J5AVCV",
    "outputId": "fd746576-560b-47da-ddc5-c525dd18e046"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter ranges\n",
    "layer1_units_range = [32, 64, 128]\n",
    "num_layer1_range = [1, 2, 3]\n",
    "layer2_units_range = [32, 64, 128]\n",
    "num_layer2_range = [1, 2, 3]\n",
    "dropout_range = [0.0, 0.25]\n",
    "num_bins_range = [4, 8, 16]\n",
    "batch_size_range = [32, 64, 128, 1024]\n",
    "\n",
    "# Define number of random search iterations\n",
    "n_iterations = 100\n",
    "best_loss = float(\"inf\")\n",
    "best_auc_global = [-np.inf]  # Use a list for best AUC to allow for updates inside the callback\n",
    "\n",
    "# Dictionary to store best hyperparameters\n",
    "best_hyperparams = {}\n",
    "\n",
    "# Now, modify the random search loop to utilize the global best AUC reference\n",
    "for i in range(n_iterations):\n",
    "    print(f\"\\n ---------- Random search iteration {i + 1}/{n_iterations} ----------\\n\")\n",
    "\n",
    "    # Randomly select hyperparameters\n",
    "    layer1_units = random.choice(layer1_units_range)\n",
    "    num_layer1 = random.choice(num_layer1_range)\n",
    "    layer2_units = random.choice(layer2_units_range)\n",
    "    num_layer2 = random.choice(num_layer2_range)\n",
    "    dropout = random.choice(dropout_range)\n",
    "    num_bins = random.choice(num_bins_range)\n",
    "    batch_size = random.choice(batch_size_range)\n",
    "\n",
    "    print(f\"Hyperparameters: \\n- layer1_units={layer1_units}, \\n- num_layer1={num_layer1}, \\n- layer2_units={layer2_units}, \\n- num_layer2={num_layer2}, \\n- dropout={dropout}, \\n- num_bins={num_bins}, \\n- batch_size={batch_size}\\n\")\n",
    "\n",
    "    # Create the model\n",
    "    model = create_model(\n",
    "        shape_text=X_enc_train.shape[1],\n",
    "        shape_num=X_num_train.shape[1],\n",
    "        layer1_values=[layer1_units] * num_layer1,\n",
    "        layer2_values=[layer2_units] * num_layer2,\n",
    "        num_bins=num_bins,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    # Callbacks for early stopping and model checkpoint\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Create the callback with a reference to the global best AUC (using list)\n",
    "    print_callback = PrintOnBestAUC(best_auc_global)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_num_train, X_enc_train], Y_train,\n",
    "        validation_data=([X_num_val, X_enc_val], Y_val),\n",
    "        epochs=20,  # You can adjust the number of epochs\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, print_callback],\n",
    "        verbose=0  # Set verbose to 0 to suppress default logging\n",
    "    )\n",
    "\n",
    "    # Get the best validation loss\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "\n",
    "    # Update the best AUC and best hyperparameters if the model improves\n",
    "    if val_loss < best_loss or best_auc_global[0] > best_hyperparams.get(\"best_auc\", -np.inf):\n",
    "        best_loss = val_loss\n",
    "        # Store the best hyperparameters\n",
    "        best_hyperparams = {\n",
    "            'layer1_units': layer1_units,\n",
    "            'num_layer1': num_layer1,\n",
    "            'layer2_units': layer2_units,\n",
    "            'num_layer2': num_layer2,\n",
    "            'dropout': dropout,\n",
    "            'num_bins': num_bins,\n",
    "            'batch_size': batch_size,\n",
    "            'best_loss': best_loss,\n",
    "            'best_auc': best_auc_global[0]\n",
    "        }\n",
    "        print(f\"Best model updated with validation loss: {best_loss:.4f} and AUC: {best_auc_global[0]:.4f}\")\n",
    "\n",
    "# Print the best hyperparameters and final results at the end of the search\n",
    "print(f\"\\nRandom search complete. Best validation loss: {best_loss:.4f}, Best AUC: {best_auc_global[0]:.4f}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparams}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "racial-bias-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
