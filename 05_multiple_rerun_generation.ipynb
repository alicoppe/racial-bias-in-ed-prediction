{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "- Load the combined dataset and configure train/test splits for multiple cohort variants.\n",
    "- Run `retraining_model_experiment` to produce paired predictions for baseline and balanced setups.\n",
    "- Save probability archives, masks, and summary tables to `experiments_data/`.\n",
    "- Generate random one-hot cohorts to validate class proportion controls.\n",
    "- Evaluate the random-control models and persist their prediction statistics for later comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rStNBaL6qm7"
   },
   "source": [
    "# Notebook Description\n",
    "Notebook used for generating the many reruns data and saving it for all data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions.notebook_utils import balance_classes, generate_one_hot, retraining_model_experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leAanMqm019Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import ttest_rel\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.pipeline import Pipeline\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "import keras.backend as K\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import h5py\n",
    "import shutil\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Rbj4e1P0_Ve",
    "outputId": "24a1aad2-a767-4b1f-86e9-e50af52a3f79"
   },
   "outputs": [],
   "source": [
    "save_dir = '/content/drive/MyDrive/Data/'\n",
    "\n",
    "save_path = os.path.join(save_dir, 'data_variables.npz')\n",
    "\n",
    "# Load variables back from the .npz file\n",
    "loaded_data = np.load(save_path, allow_pickle=True)\n",
    "# X_train = loaded_data['X_train']\n",
    "# X_test = loaded_data['X_test']\n",
    "Y_train = loaded_data['Y_train']\n",
    "Y_test = loaded_data['Y_test']\n",
    "X_train_num = loaded_data['X_train_num']\n",
    "X_test_num = loaded_data['X_test_num']\n",
    "X_train_encoded = loaded_data['X_train_encoded']\n",
    "X_test_encoded = loaded_data['X_test_encoded']\n",
    "\n",
    "races = ['ASIAN', 'BLACK', 'HISPANIC', 'OTHER', 'WHITE']\n",
    "\n",
    "print(\"Variables loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab8PFEG17LMB"
   },
   "source": [
    "# Baseline Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SDkLraJ4xFi",
    "outputId": "5d3e80b6-f54b-454f-b17c-6cf69bc0f8eb"
   },
   "outputs": [],
   "source": [
    "masks_hdf5_name = 'baseline_masks.h5'\n",
    "google_drive_save_path = save_dir + masks_hdf5_name\n",
    "\n",
    "loaded_masks = {}\n",
    "\n",
    "with h5py.File(google_drive_save_path, 'r') as hdf:\n",
    "    for race in hdf.keys():  # Iterate over the keys (which are the race names)\n",
    "        loaded_masks[race] = np.array(hdf[race])  # Load each dataset as a NumPy array\n",
    "\n",
    "masks = loaded_masks\n",
    "\n",
    "print(f\"Successfully loaded masks from path: {google_drive_save_path}\")\n",
    "\n",
    "mean_differences, p_value_tables = retraining_model_experiment(X_train_num, X_train_encoded, Y_train,\n",
    "                                                               X_test_num, X_test_encoded, Y_test,\n",
    "                                                               masks, races, save_name = 'baseline',\n",
    "                                                               num_reruns=50, p_values=[0.05, 0.01, 0.001, 0.0001],\n",
    "                                                               save_increment=5, epochs=3, batch_size=32, seed=42,\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6MNqhXD7Nu_"
   },
   "source": [
    "# Balanced Data Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "GEUe-KapKDst",
    "outputId": "e50dc508-fd11-4c64-cad3-0e683061d22f"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "balanced_data_save_path = os.path.join(save_dir, 'balanced_data.npz')\n",
    "masks_hdf5_name = 'balanced_masks.h5'\n",
    "google_drive_masks_path = os.path.join(save_dir, masks_hdf5_name)\n",
    "\n",
    "if os.path.exists(balanced_data_save_path) and os.path.exists(google_drive_masks_path):\n",
    "    # Load existing files\n",
    "    print(\"Balanced dataset and masks already exist. Loading...\")\n",
    "\n",
    "    # Load balanced dataset\n",
    "    loaded_data = np.load(balanced_data_save_path)\n",
    "    X_train_num_bal = loaded_data['X_train_num_bal']\n",
    "    X_train_encoded_bal = loaded_data['X_train_encoded_bal']\n",
    "    Y_train_bal = loaded_data['Y_train_bal']\n",
    "    X_test_num_bal = loaded_data['X_test_num_bal']\n",
    "    X_test_encoded_bal = loaded_data['X_test_encoded_bal']\n",
    "    Y_test_bal = loaded_data['Y_test_bal']\n",
    "\n",
    "    # Load masks\n",
    "    loaded_masks = {}\n",
    "    with h5py.File(google_drive_masks_path, 'r') as hdf:\n",
    "        for race in hdf.keys():\n",
    "            loaded_masks[race] = np.array(hdf[race])\n",
    "    masks_bal = loaded_masks\n",
    "\n",
    "    print(\"Balanced dataset and masks loaded successfully.\")\n",
    "else:\n",
    "    # Create and save balanced dataset and masks\n",
    "    print(\"Balanced dataset and masks do not exist. Creating...\")\n",
    "\n",
    "    # Balance the datasets\n",
    "    X_train_num_bal, X_train_encoded_bal, Y_train_bal = balance_classes(X_train_num, X_train_encoded, Y_train)\n",
    "    X_test_num_bal, X_test_encoded_bal, Y_test_bal = balance_classes(X_test_num, X_test_encoded, Y_test)\n",
    "\n",
    "    # Save balanced dataset\n",
    "    np.savez(\n",
    "        balanced_data_save_path,\n",
    "        X_train_num_bal=X_train_num_bal,\n",
    "        X_train_encoded_bal=X_train_encoded_bal,\n",
    "        Y_train_bal=Y_train_bal,\n",
    "        X_test_num_bal=X_test_num_bal,\n",
    "        X_test_encoded_bal=X_test_encoded_bal,\n",
    "        Y_test_bal=Y_test_bal\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Balanced dataset saved to {balanced_data_save_path}\")\n",
    "\n",
    "    # Create and save masks\n",
    "    masks_bal = {race: X_test_num_bal[:, -5+j] == 1 for j, race in enumerate(races)}\n",
    "    with h5py.File(masks_hdf5_name, 'w') as hdf:\n",
    "        for race, mask in masks_bal.items():\n",
    "            hdf.create_dataset(race, data=np.stack(mask))\n",
    "\n",
    "    shutil.move(masks_hdf5_name, google_drive_masks_path)\n",
    "    print(f\"Masks saved to {google_drive_masks_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "CB6PSQsdjHP5",
    "outputId": "157309f5-33fd-4f7d-d141-b7b4b218556e"
   },
   "outputs": [],
   "source": [
    "mean_differences, p_value_tables = retraining_model_experiment(X_train_num_bal, X_train_encoded_bal, Y_train_bal,\n",
    "                                                               X_test_num_bal, X_test_encoded_bal, Y_test_bal,\n",
    "                                                               masks_bal, races, save_name = 'balanced',\n",
    "                                                               num_reruns=50, p_values=[0.05, 0.01, 0.001, 0.0001],\n",
    "                                                               save_increment=5, epochs=1, batch_size=32, seed=42,\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd6URb0A7fKC"
   },
   "source": [
    "# Model Evaluation with Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTNvQcSH7iiK"
   },
   "outputs": [],
   "source": [
    "proportions = [0.04483654119629676,0.22351021102519597,0.0834425362363142,0.06299656309889928,0.5852141484432938]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RaNz7tl-nns",
    "outputId": "6d5c0f5b-0657-46f7-8909-68eb3366d826"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "masks_hdf5_name = 'random_masks.h5'\n",
    "google_drive_masks_path = os.path.join(save_dir, masks_hdf5_name)\n",
    "data_save_path = os.path.join(save_dir, 'random_data.npz')\n",
    "\n",
    "if os.path.exists(google_drive_masks_path) and os.path.exists(data_save_path):\n",
    "    # Load existing files\n",
    "    print(\"Files already exist. Loading...\")\n",
    "\n",
    "    # Load masks\n",
    "    loaded_masks = {}\n",
    "    with h5py.File(google_drive_masks_path, 'r') as hdf:\n",
    "        for race in hdf.keys():\n",
    "            loaded_masks[race] = np.array(hdf[race])\n",
    "\n",
    "    rand_masks = loaded_masks\n",
    "\n",
    "    # Load data\n",
    "    loaded_data = np.load(data_save_path)\n",
    "    X_train_num_rand = loaded_data['X_train_num_rand']\n",
    "    X_test_num_rand = loaded_data['X_test_num_rand']\n",
    "\n",
    "    print(\"Random masks and data loaded successfully.\")\n",
    "else:\n",
    "    # Create new files\n",
    "    print(\"Files do not exist. Creating...\")\n",
    "\n",
    "    randomn_data = generate_one_hot(proportions, X_train_num.shape[0] + X_test_num.shape[0])\n",
    "\n",
    "    X_train_num_rand = X_train_num.copy()\n",
    "    X_train_num_rand[:, -5:] = randomn_data[:X_train_num.shape[0]]\n",
    "    X_test_num_rand = X_test_num.copy()\n",
    "    X_test_num_rand[:, -5:] = randomn_data[X_train_num.shape[0]:]\n",
    "\n",
    "    # Prepare race masks\n",
    "    rand_masks = {race: X_test_num_rand[:, -5 + j] == 1 for j, race in enumerate(races)}\n",
    "\n",
    "    # Save masks in HDF5 format\n",
    "    with h5py.File(masks_hdf5_name, 'w') as hdf:\n",
    "        for race in races:\n",
    "            hdf.create_dataset(race, data=np.stack(rand_masks[race]))  # Stack masks into a single array per race\n",
    "\n",
    "    # Move the HDF5 file to Google Drive\n",
    "    shutil.move(masks_hdf5_name, google_drive_masks_path)\n",
    "    print(f'HDF5 file with masks saved to {google_drive_masks_path}')\n",
    "\n",
    "    # Save X_train_num_rand and X_test_num_rand in an NPZ file\n",
    "    np.savez(\n",
    "        data_save_path,\n",
    "        X_train_num_rand=X_train_num_rand,\n",
    "        X_test_num_rand=X_test_num_rand\n",
    "    )\n",
    "    print(f'Random data saved to {data_save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pa0aMo0ijZTZ",
    "outputId": "c362ca07-71a6-4650-de01-0fee794e10d4"
   },
   "outputs": [],
   "source": [
    "mean_differences, p_value_tables = retraining_model_experiment(X_train_num_rand, X_train_encoded, Y_train,\n",
    "                                                               X_test_num_rand, X_test_encoded, Y_test,\n",
    "                                                               rand_masks, races, save_name = 'random',\n",
    "                                                               num_reruns=50, p_values=[0.05, 0.01, 0.001, 0.0001],\n",
    "                                                               save_increment=5, epochs=3, batch_size=32, seed=42,\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_MjFx6P_BnZ"
   },
   "source": [
    "# Random Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rv_1OLSJ_Mo5",
    "outputId": "25e7205a-9af8-4558-9a76-b826e1e5edf6"
   },
   "outputs": [],
   "source": [
    "save_dir = '/content/drive/MyDrive/Data/'\n",
    "\n",
    "\n",
    "save_path = os.path.join(save_dir, 'data_variables.npz')\n",
    "\n",
    "# Load variables back from the .npz file\n",
    "loaded_data = np.load(save_path, allow_pickle=True)\n",
    "# X_train = loaded_data['X_train']\n",
    "# X_test = loaded_data['X_test']\n",
    "Y_train = loaded_data['Y_train']\n",
    "Y_test = loaded_data['Y_test']\n",
    "X_train_num = loaded_data['X_train_num']\n",
    "X_test_num = loaded_data['X_test_num']\n",
    "X_train_encoded = loaded_data['X_train_encoded']\n",
    "X_test_encoded = loaded_data['X_test_encoded']\n",
    "\n",
    "races = ['ASIAN', 'BLACK', 'HISPANIC', 'OTHER', 'WHITE']\n",
    "\n",
    "print(\"Original data variables loaded successfully\")\n",
    "\n",
    "\n",
    "save_path = os.path.join(save_dir, 'random_data.npz')\n",
    "\n",
    "# Load variables back from the .npz file\n",
    "loaded_data = np.load(save_path, allow_pickle=True)\n",
    "# X_train = loaded_data['X_train']\n",
    "# X_test = loaded_data['X_test']\n",
    "\n",
    "X_train_num_rand = loaded_data['X_train_num_rand']\n",
    "X_test_num_rand = loaded_data['X_test_num_rand']\n",
    "\n",
    "print(\"Random data variables loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3kgTf3OGOL8"
   },
   "source": [
    "## Loading the random balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnqXDckRB-9g",
    "outputId": "cb4c6cab-332a-4e45-8a5e-cfc34b8af861"
   },
   "outputs": [],
   "source": [
    "random_balanced_data_save_path = os.path.join(save_dir, 'random_balanced_data.npz')\n",
    "masks_hdf5_name = 'random_balanced_masks.h5'\n",
    "google_drive_masks_path = os.path.join(save_dir, masks_hdf5_name)\n",
    "\n",
    "if not os.path.exists(random_balanced_data_save_path) and not os.path.exists(google_drive_masks_path):\n",
    "    X_train_num_rand_bal, X_train_enc_rand_bal, Y_train_rand_bal = balance_classes(X_train_num_rand, X_train_encoded, Y_train)\n",
    "    X_test_num_rand_bal, X_test_enc_rand_bal, Y_test_rand_bal = balance_classes(X_test_num_rand, X_test_encoded, Y_test)\n",
    "\n",
    "    np.savez(\n",
    "        random_balanced_data_save_path,\n",
    "        X_train_num_rand_bal=X_train_num_rand_bal,\n",
    "        X_train_enc_rand_bal=X_train_enc_rand_bal,\n",
    "        Y_train_rand_bal=Y_train_rand_bal,\n",
    "        X_test_num_rand_bal=X_test_num_rand_bal,\n",
    "        X_test_enc_rand_bal=X_test_enc_rand_bal,\n",
    "        Y_test_rand_bal=Y_test_rand_bal\n",
    "    )\n",
    "\n",
    "    print(f\"Random balanced dataset saved to {random_balanced_data_save_path}\")\n",
    "\n",
    "    # Create and save masks\n",
    "    masks_rand_bal = {race: X_test_num_rand_bal[:, -5+j] == 1 for j, race in enumerate(races)}\n",
    "    with h5py.File(masks_hdf5_name, 'w') as hdf:\n",
    "        for race, mask in masks_rand_bal.items():\n",
    "            hdf.create_dataset(race, data=np.stack(mask))\n",
    "\n",
    "    shutil.move(masks_hdf5_name, google_drive_masks_path)\n",
    "    print(f\"Masks saved to {google_drive_masks_path}\")\n",
    "\n",
    "else:\n",
    "    # Load existing files\n",
    "    print(\"Random balanced dataset and masks already exist. Loading...\")\n",
    "\n",
    "    # Load random balanced dataset\n",
    "    loaded_data = np.load(random_balanced_data_save_path)\n",
    "    X_train_num_rand_bal = loaded_data['X_train_num_rand_bal']\n",
    "    X_train_enc_rand_bal = loaded_data['X_train_enc_rand_bal']\n",
    "    Y_train_rand_bal = loaded_data['Y_train_rand_bal']\n",
    "    X_test_num_rand_bal = loaded_data['X_test_num_rand_bal']\n",
    "    X_test_enc_rand_bal = loaded_data['X_test_enc_rand_bal']\n",
    "    Y_test_rand_bal = loaded_data['Y_test_rand_bal']\n",
    "\n",
    "    # Load masks\n",
    "    loaded_masks = {}\n",
    "    with h5py.File(google_drive_masks_path, 'r') as hdf:\n",
    "        for race in hdf.keys():\n",
    "            loaded_masks[race] = np.array(hdf[race])\n",
    "    masks_rand_bal = loaded_masks\n",
    "\n",
    "    print(\"Random balanced dataset and masks loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "q22-owlLGrEp",
    "outputId": "2d0a46d2-6f83-4298-dbb0-e1e6029476c3"
   },
   "outputs": [],
   "source": [
    "mean_differences, p_value_tables = retraining_model_experiment(X_train_num_rand_bal, X_train_enc_rand_bal, Y_train_rand_bal,\n",
    "                                                               X_test_num_rand_bal, X_test_enc_rand_bal, Y_test_rand_bal,\n",
    "                                                               masks_rand_bal, races, save_name='rand_bal',\n",
    "                                                               num_reruns=50, p_values=[0.05, 0.01, 0.001, 0.0001],\n",
    "                                                               save_increment=3, epochs=1, batch_size=32, seed=42,\n",
    "                                                               )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
